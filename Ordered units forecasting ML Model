import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from sklearn.preprocessing import MinMaxScaler
import datetime

# Load the data
file_path = r'C:\Users\kossures\Documents\temp_junk\Ordered_units_at_the_1732456777821.csv'  # Replace with your file path
data = pd.read_csv(file_path)

# Preprocess the data
data['order_day'] = pd.to_datetime(data['order_day'])
data.sort_values('order_day', inplace=True)

# Aggregate data by pl_new, gl_name, and order_day
grouped_data = data.groupby(['order_day', 'pl_new', 'gl_name'])['ordered_units'].sum().reset_index()

# Normalize ordered_units for each grain
scaler = MinMaxScaler(feature_range=(0, 1))
grouped_data['normalized_units'] = scaler.fit_transform(grouped_data[['ordered_units']])

# Create a dictionary to store features per grain
features_per_grain = {}

# Prepare features and labels for each grain
look_back = 30  # Look-back period
for grain, group in grouped_data.groupby(['pl_new', 'gl_name']):
    grain_key = f"{grain[0]}_{grain[1]}"
    data_points = group[['order_day', 'normalized_units']].sort_values('order_day').reset_index(drop=True)
    
    X, y = [], []
    for i in range(look_back, len(data_points)):
        X.append(data_points['normalized_units'].iloc[i-look_back:i].values)
        y.append(data_points['normalized_units'].iloc[i])
    
    # Skip grains without enough data
    if len(X) > 0 and len(y) > 0:
        features_per_grain[grain_key] = {
            'X': np.array(X),
            'y': np.array(y),
            'dates': data_points['order_day'].iloc[look_back:],
            'original_data': group
        }

# Combine all grains for training
X_combined, y_combined = [], []
for grain_key, data in features_per_grain.items():
    # Ensure valid data dimensions
    if data['X'].ndim == 2:  # Reshape to 3D if necessary
        data['X'] = data['X'].reshape(data['X'].shape[0], data['X'].shape[1], 1)
    if data['y'].ndim == 1:  # Reshape to 2D if necessary
        data['y'] = data['y'].reshape(-1, 1)
    
    # Append to combined lists
    X_combined.append(data['X'])
    y_combined.append(data['y'])

# Concatenate all valid data
if X_combined and y_combined:
    X_combined = np.concatenate(X_combined, axis=0)  # Shape: (total_samples, look_back, 1)
    y_combined = np.concatenate(y_combined, axis=0)  # Shape: (total_samples, 1)
else:
    raise ValueError("No valid data available for training. Check preprocessing.")

# Split data into training and testing
split_ratio = 0.8
train_size = int(len(X_combined) * split_ratio)
X_train, X_test = X_combined[:train_size], X_combined[train_size:]
y_train, y_test = y_combined[:train_size], y_combined[train_size:]

# Build the LSTM model
model = Sequential([
    LSTM(64, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], 1)),
    Dropout(0.2),
    LSTM(32, activation='relu'),
    Dropout(0.2),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse')

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# Generate forecast for the next 30 days
def generate_forecast(model, features_per_grain, scaler, look_back=30, days=30):
    forecast_results = []
    for grain_key, data in features_per_grain.items():
        last_data = data['X'][-1]  # Last sequence of normalized data
        forecasts = []
        input_seq = last_data.copy()
        
        for _ in range(days):
            pred = model.predict(input_seq.reshape(1, input_seq.shape[0], 1))
            forecasts.append(pred[0][0])
            input_seq = np.append(input_seq[1:], pred)
        
        # Inverse scale the forecasts
        forecasts = scaler.inverse_transform(np.array(forecasts).reshape(-1, 1)).flatten()
        
        # Save results with grain and dates
        forecast_dates = [data['dates'].max() + datetime.timedelta(days=i+1) for i in range(days)]
        grain_forecast = pd.DataFrame({
            'order_day': forecast_dates,
            'pl_new': grain_key.split('_')[0],
            'gl_name': grain_key.split('_')[1],
            'ordered_units': forecasts
        })
        forecast_results.append(grain_forecast)
    
    # Combine all grain forecasts
    return pd.concat(forecast_results, ignore_index=True)

# Generate the forecast
forecast_output = generate_forecast(model, features_per_grain, scaler)

# Save to CSV
file_path = r"C:\Users\kossures\Documents\temp_junk\forecast_output.csv"  # Specify the full path
forecast_output.to_csv(file_path, index=False)
print(f"Forecast saved to '{file_path}'")
